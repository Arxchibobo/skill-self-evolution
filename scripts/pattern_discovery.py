#!/usr/bin/env python3
"""
Pattern Discovery - æ¨¡å¼å‘ç°å™¨

ä½¿ç”¨ Apriori ç®—æ³•å‘ç°å…ƒç´ çš„é¢‘ç¹ç»„åˆæ¨¡å¼ï¼Œè¯†åˆ«æˆåŠŸå’Œå¤±è´¥çš„æ¨¡å¼ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é¢‘ç¹é¡¹é›†æŒ–æ˜ - å‘ç°é«˜é¢‘å…ƒç´ ç»„åˆ
2. æˆåŠŸæ¨¡å¼è¯†åˆ« - è¯†åˆ«å¯¼è‡´é«˜è´¨é‡è¾“å‡ºçš„æ¨¡å¼
3. å¤±è´¥æ¨¡å¼è¯†åˆ« - è¯†åˆ«åº”é¿å…çš„åæ¨¡å¼
4. å…³è”è§„åˆ™ç”Ÿæˆ - A => B ç±»å‹çš„è§„åˆ™
5. è·¨åŸŸæ¨¡å¼è¿ç§» - è¯†åˆ«å¯è¿ç§»çš„æ¨¡å¼

ç®—æ³•ï¼š
Apriori: L_k = {c âˆˆ C_k | support(c) â‰¥ min_support}
å…¶ä¸­ C_k = apriori_gen(L_{k-1})

Author: Bobo (Self-Evolution Skill)
"""

import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
from collections import defaultdict, Counter
from itertools import combinations

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))


class PatternDiscovery:
    """æ¨¡å¼å‘ç°å™¨ä¸»ç±»"""

    def __init__(self, min_support: float = 0.1, min_confidence: float = 0.7,
                 quality_threshold: float = 0.8):
        """åˆå§‹åŒ–æ¨¡å¼å‘ç°å™¨

        Args:
            min_support: æœ€å°æ”¯æŒåº¦ï¼ˆ0-1ï¼‰
            min_confidence: æœ€å°ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
            quality_threshold: è´¨é‡åˆ†æ•°é˜ˆå€¼
        """
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.quality_threshold = quality_threshold

        self.data_dir = PROJECT_ROOT / 'data'
        self.executions_dir = self.data_dir / 'executions'
        self.patterns_dir = self.data_dir / 'patterns'
        self.patterns_dir.mkdir(exist_ok=True)

    def load_executions(self, days: int = 30) -> List[Dict]:
        """åŠ è½½æ‰§è¡Œè®°å½•

        Args:
            days: åŠ è½½æœ€è¿‘ N å¤©çš„æ•°æ®

        Returns:
            æ‰§è¡Œè®°å½•åˆ—è¡¨
        """
        executions = []
        cutoff_date = datetime.now() - timedelta(days=days)

        if not self.executions_dir.exists():
            return executions

        for file_path in self.executions_dir.glob('*.json'):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    timestamp = datetime.fromisoformat(data.get('timestamp', '').replace('Z', '+00:00'))
                    if timestamp >= cutoff_date:
                        executions.append(data)
            except Exception as e:
                print(f"Warning: Failed to load {file_path}: {e}")

        return executions

    def extract_transactions(self, executions: List[Dict]) -> List[Tuple[Set[str], float, Dict]]:
        """æå–äº‹åŠ¡ï¼ˆæ¯ä¸ªæ‰§è¡Œè®°å½•ä½œä¸ºä¸€ä¸ªäº‹åŠ¡ï¼‰

        Args:
            executions: æ‰§è¡Œè®°å½•åˆ—è¡¨

        Returns:
            [(itemset, quality_score, metadata), ...] åˆ—è¡¨
        """
        transactions = []

        for execution in executions:
            # æå–ä½¿ç”¨çš„å…ƒç´ 
            items = set()
            elements_used = execution.get('elements_used', {})

            for category, values in elements_used.items():
                for value in values:
                    # ç®€åŒ–å½¢å¼ï¼šåªä¿ç•™å€¼ï¼Œä¸å¸¦åˆ†ç±»å‰ç¼€
                    items.add(value)

            # è·å–è´¨é‡åˆ†æ•°
            quality_scores = execution.get('quality_scores', {})
            overall_quality = quality_scores.get('overall', 0.0)

            # å…ƒæ•°æ®
            metadata = {
                'session_id': execution.get('session_id'),
                'skill_name': execution.get('skill_name'),
                'timestamp': execution.get('timestamp'),
                'searches': execution.get('searches_performed', [])
            }

            transactions.append((items, overall_quality, metadata))

        return transactions

    def find_frequent_itemsets(self, transactions: List[Tuple[Set[str], float, Dict]],
                               max_length: int = 3) -> Dict[int, List[Tuple[frozenset, int]]]:
        """ä½¿ç”¨ Apriori ç®—æ³•æŸ¥æ‰¾é¢‘ç¹é¡¹é›†

        Args:
            transactions: äº‹åŠ¡åˆ—è¡¨
            max_length: æœ€å¤§é¡¹é›†é•¿åº¦

        Returns:
            {length: [(itemset, support_count), ...]} å­—å…¸
        """
        # æå–é¡¹é›†ï¼ˆå¿½ç•¥è´¨é‡åˆ†æ•°ï¼‰
        itemsets = [itemset for itemset, _, _ in transactions]
        n_transactions = len(itemsets)

        # æœ€å°æ”¯æŒè®¡æ•°
        min_support_count = int(self.min_support * n_transactions)

        # L1: å•é¡¹é¢‘ç¹é›†
        item_counts = Counter()
        for itemset in itemsets:
            for item in itemset:
                item_counts[item] += 1

        L1 = [(frozenset([item]), count) for item, count in item_counts.items()
              if count >= min_support_count]

        # å­˜å‚¨æ‰€æœ‰é¢‘ç¹é¡¹é›†
        all_frequent = {1: L1}

        # ç”Ÿæˆ L2, L3, ... Lk
        current_L = L1
        k = 2

        while current_L and k <= max_length:
            # ç”Ÿæˆå€™é€‰é›† Ck
            candidates = self._apriori_gen(current_L, k)

            # è®¡æ•°
            candidate_counts = defaultdict(int)
            for itemset in itemsets:
                for candidate in candidates:
                    if candidate.issubset(itemset):
                        candidate_counts[candidate] += 1

            # è¿‡æ»¤ï¼šä¿ç•™é¢‘ç¹é¡¹é›†
            current_L = [(itemset, count) for itemset, count in candidate_counts.items()
                        if count >= min_support_count]

            if current_L:
                all_frequent[k] = current_L

            k += 1

        return all_frequent

    def _apriori_gen(self, L_prev: List[Tuple[frozenset, int]], k: int) -> Set[frozenset]:
        """Apriori å€™é€‰ç”Ÿæˆå‡½æ•°

        ä» L_{k-1} ç”Ÿæˆ C_k

        Args:
            L_prev: L_{k-1} é¢‘ç¹é¡¹é›†
            k: ç›®æ ‡é•¿åº¦

        Returns:
            C_k å€™é€‰é›†
        """
        candidates = set()
        items = [itemset for itemset, _ in L_prev]

        # è¿æ¥æ­¥éª¤ï¼šL_{k-1} Ã— L_{k-1}
        for i in range(len(items)):
            for j in range(i + 1, len(items)):
                # åˆå¹¶ä¸¤ä¸ªé¡¹é›†
                union = items[i] | items[j]

                # å¦‚æœåˆå¹¶åé•¿åº¦ä¸º kï¼Œæ·»åŠ åˆ°å€™é€‰é›†
                if len(union) == k:
                    candidates.add(union)

        # å‰ªææ­¥éª¤ï¼šåˆ é™¤åŒ…å«éé¢‘ç¹å­é›†çš„å€™é€‰
        pruned_candidates = set()
        frequent_subsets = set(items)

        for candidate in candidates:
            # æ£€æŸ¥æ‰€æœ‰ k-1 å­é›†æ˜¯å¦éƒ½é¢‘ç¹
            subsets = [frozenset(s) for s in combinations(candidate, k - 1)]
            if all(subset in frequent_subsets for subset in subsets):
                pruned_candidates.add(candidate)

        return pruned_candidates

    def identify_success_patterns(self, transactions: List[Tuple[Set[str], float, Dict]],
                                   frequent_itemsets: Dict[int, List[Tuple[frozenset, int]]]) -> List[Dict]:
        """è¯†åˆ«æˆåŠŸæ¨¡å¼

        Args:
            transactions: äº‹åŠ¡åˆ—è¡¨
            frequent_itemsets: é¢‘ç¹é¡¹é›†

        Returns:
            æˆåŠŸæ¨¡å¼åˆ—è¡¨
        """
        success_patterns = []
        n_transactions = len(transactions)

        # éå†æ‰€æœ‰é¢‘ç¹é¡¹é›†
        for length, itemsets in frequent_itemsets.items():
            for itemset, count in itemsets:
                # è®¡ç®—åŒ…å«è¯¥é¡¹é›†çš„äº‹åŠ¡çš„å¹³å‡è´¨é‡åˆ†æ•°
                matching_transactions = [
                    (quality, metadata)
                    for items, quality, metadata in transactions
                    if itemset.issubset(items)
                ]

                if not matching_transactions:
                    continue

                avg_quality = sum(q for q, _ in matching_transactions) / len(matching_transactions)

                # å¦‚æœå¹³å‡è´¨é‡é«˜äºé˜ˆå€¼ï¼Œè¯†åˆ«ä¸ºæˆåŠŸæ¨¡å¼
                if avg_quality >= self.quality_threshold:
                    pattern = {
                        'items': list(itemset),
                        'support': count / n_transactions,
                        'occurrence_count': count,
                        'avg_quality_score': round(avg_quality, 3),
                        'confidence': len(matching_transactions) / count if count > 0 else 0,
                        'sample_sessions': [m['session_id'] for _, m in matching_transactions[:3]]
                    }
                    success_patterns.append(pattern)

        # æŒ‰å¹³å‡è´¨é‡åˆ†æ•°æ’åº
        success_patterns.sort(key=lambda x: x['avg_quality_score'], reverse=True)

        return success_patterns

    def identify_failure_patterns(self, transactions: List[Tuple[Set[str], float, Dict]],
                                   frequent_itemsets: Dict[int, List[Tuple[frozenset, int]]]) -> List[Dict]:
        """è¯†åˆ«å¤±è´¥æ¨¡å¼ï¼ˆåæ¨¡å¼ï¼‰

        Args:
            transactions: äº‹åŠ¡åˆ—è¡¨
            frequent_itemsets: é¢‘ç¹é¡¹é›†

        Returns:
            å¤±è´¥æ¨¡å¼åˆ—è¡¨
        """
        failure_patterns = []
        n_transactions = len(transactions)
        failure_threshold = 0.6  # ä½äºæ­¤åˆ†æ•°è§†ä¸ºå¤±è´¥

        # éå†æ‰€æœ‰é¢‘ç¹é¡¹é›†
        for length, itemsets in frequent_itemsets.items():
            for itemset, count in itemsets:
                # è®¡ç®—åŒ…å«è¯¥é¡¹é›†çš„äº‹åŠ¡
                matching_transactions = [
                    (quality, metadata)
                    for items, quality, metadata in transactions
                    if itemset.issubset(items)
                ]

                if not matching_transactions:
                    continue

                avg_quality = sum(q for q, _ in matching_transactions) / len(matching_transactions)

                # å¦‚æœå¹³å‡è´¨é‡ä½äºå¤±è´¥é˜ˆå€¼ï¼Œè¯†åˆ«ä¸ºå¤±è´¥æ¨¡å¼
                if avg_quality < failure_threshold:
                    pattern = {
                        'pattern_type': 'anti_pattern',
                        'elements': list(itemset),
                        'failure_rate': 1 - avg_quality,
                        'avg_quality_score': round(avg_quality, 3),
                        'sample_size': len(matching_transactions)
                    }
                    failure_patterns.append(pattern)

        # æŒ‰å¤±è´¥ç‡æ’åº
        failure_patterns.sort(key=lambda x: x['failure_rate'], reverse=True)

        return failure_patterns

    def identify_search_sequences(self, transactions: List[Tuple[Set[str], float, Dict]]) -> List[Dict]:
        """è¯†åˆ«æˆåŠŸçš„æœç´¢åºåˆ—æ¨¡å¼

        Args:
            transactions: äº‹åŠ¡åˆ—è¡¨

        Returns:
            æœç´¢åºåˆ—æ¨¡å¼åˆ—è¡¨
        """
        sequence_patterns = defaultdict(lambda: {'qualities': [], 'count': 0})

        for items, quality, metadata in transactions:
            searches = metadata.get('searches', [])
            if not searches:
                continue

            # æå–æœç´¢åŸŸåºåˆ—
            sequence = tuple([s.get('domain') for s in searches if s.get('domain')])

            if sequence:
                sequence_patterns[sequence]['qualities'].append(quality)
                sequence_patterns[sequence]['count'] += 1

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        patterns = []
        for sequence, data in sequence_patterns.items():
            if data['count'] >= 3:  # è‡³å°‘å‡ºç° 3 æ¬¡
                avg_quality = sum(data['qualities']) / len(data['qualities'])
                success_rate = sum(1 for q in data['qualities'] if q >= self.quality_threshold) / len(data['qualities'])

                if success_rate >= 0.7:  # 70% æˆåŠŸç‡
                    patterns.append({
                        'pattern_type': 'search_sequence',
                        'sequence': list(sequence),
                        'success_rate': round(success_rate, 3),
                        'avg_quality_score': round(avg_quality, 3),
                        'sample_size': data['count']
                    })

        # æŒ‰æˆåŠŸç‡æ’åº
        patterns.sort(key=lambda x: x['success_rate'], reverse=True)

        return patterns

    def discover(self, days: int = 30, verbose: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨¡å¼å‘ç°

        Args:
            days: åˆ†ææœ€è¿‘ N å¤©çš„æ•°æ®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            å‘ç°ç»“æœæ‘˜è¦
        """
        if verbose:
            print(f"ğŸ” Starting pattern discovery (analyzing last {days} days)...")

        # 1. åŠ è½½æ•°æ®
        executions = self.load_executions(days)

        if not executions:
            print("âŒ No execution data found. Skipping pattern discovery.")
            return {'status': 'no_data'}

        if verbose:
            print(f"   Loaded {len(executions)} executions")

        # 2. æå–äº‹åŠ¡
        transactions = self.extract_transactions(executions)

        if verbose:
            print(f"   Extracted {len(transactions)} transactions")

        # 3. æŸ¥æ‰¾é¢‘ç¹é¡¹é›†
        if verbose:
            print("ğŸ“Š Finding frequent itemsets...")

        frequent_itemsets = self.find_frequent_itemsets(transactions, max_length=3)

        total_frequent = sum(len(itemsets) for itemsets in frequent_itemsets.values())
        if verbose:
            print(f"   Found {total_frequent} frequent itemsets")

        # 4. è¯†åˆ«æˆåŠŸæ¨¡å¼
        if verbose:
            print("âœ… Identifying success patterns...")

        success_patterns = self.identify_success_patterns(transactions, frequent_itemsets)

        if verbose:
            print(f"   Found {len(success_patterns)} success patterns")

        # 5. è¯†åˆ«å¤±è´¥æ¨¡å¼
        if verbose:
            print("âš ï¸  Identifying failure patterns...")

        failure_patterns = self.identify_failure_patterns(transactions, frequent_itemsets)

        if verbose:
            print(f"   Found {len(failure_patterns)} failure patterns")

        # 6. è¯†åˆ«æœç´¢åºåˆ—
        if verbose:
            print("ğŸ”„ Identifying search sequence patterns...")

        search_sequences = self.identify_search_sequences(transactions)

        if verbose:
            print(f"   Found {len(search_sequences)} search sequence patterns")

        # 7. ä¿å­˜ç»“æœ
        result = {
            'generated_at': datetime.now().isoformat(),
            'period': {
                'start_date': (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'),
                'end_date': datetime.now().strftime('%Y-%m-%d'),
                'days': days
            },
            'frequent_combinations': success_patterns[:10],  # Top 10
            'success_patterns': search_sequences[:5],  # Top 5
            'failure_patterns': failure_patterns[:5],  # Top 5
            'statistics': {
                'total_executions': len(executions),
                'total_frequent_itemsets': total_frequent,
                'success_patterns_count': len(success_patterns),
                'failure_patterns_count': len(failure_patterns),
                'search_sequence_patterns': len(search_sequences)
            }
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'patterns_{timestamp}.json'
        filepath = self.patterns_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        # åˆ›å»º latest.json
        latest_path = self.patterns_dir / 'latest.json'
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        if verbose:
            print(f"âœ… Pattern discovery complete! Saved to: {filepath}")

        return {
            'status': 'success',
            'file_path': str(filepath),
            'success_patterns': len(success_patterns),
            'failure_patterns': len(failure_patterns)
        }


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(
        description='Pattern Discovery - å‘ç° Self-Evolution Skill çš„ä½¿ç”¨æ¨¡å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†ææœ€è¿‘ 30 å¤©æ•°æ®
  python pattern_discovery.py

  # åˆ†ææœ€è¿‘ 90 å¤©æ•°æ®
  python pattern_discovery.py --days 90

  # è°ƒæ•´æ”¯æŒåº¦å’Œç½®ä¿¡åº¦
  python pattern_discovery.py --min-support 0.15 --min-confidence 0.8
        """
    )

    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='åˆ†ææœ€è¿‘ N å¤©çš„æ•°æ® (é»˜è®¤: 30)'
    )

    parser.add_argument(
        '--min-support',
        type=float,
        default=0.1,
        help='æœ€å°æ”¯æŒåº¦ (0-1, é»˜è®¤: 0.1)'
    )

    parser.add_argument(
        '--min-confidence',
        type=float,
        default=0.7,
        help='æœ€å°ç½®ä¿¡åº¦ (0-1, é»˜è®¤: 0.7)'
    )

    parser.add_argument(
        '--quality-threshold',
        type=float,
        default=0.8,
        help='è´¨é‡åˆ†æ•°é˜ˆå€¼ (0-1, é»˜è®¤: 0.8)'
    )

    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼'
    )

    args = parser.parse_args()

    try:
        discovery = PatternDiscovery(
            min_support=args.min_support,
            min_confidence=args.min_confidence,
            quality_threshold=args.quality_threshold
        )

        result = discovery.discover(
            days=args.days,
            verbose=not args.quiet
        )

        if result['status'] == 'success':
            sys.exit(0)
        else:
            sys.exit(1)

    except Exception as e:
        print(f"âŒ Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
