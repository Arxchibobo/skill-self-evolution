#!/usr/bin/env python3
"""
Self-Evolution Skill - å‘¨æŠ¥ç”Ÿæˆå·¥å…·

ç”Ÿæˆæ¯å‘¨è¿›åŒ–æŠ¥å‘Šï¼Œå±•ç¤ºè´¨é‡è¶‹åŠ¿ã€å‘ç°çš„æ¨¡å¼ã€æƒé‡å˜åŒ–ç­‰ã€‚

ç”¨æ³•:
    python weekly_report.py                    # ç”Ÿæˆä¸Šå‘¨æŠ¥å‘Š
    python weekly_report.py --weeks 4          # ç”Ÿæˆæœ€è¿‘ 4 å‘¨æŠ¥å‘Š
    python weekly_report.py --output weekly.md # æŒ‡å®šè¾“å‡ºæ–‡ä»¶

è¾“å‡º:
    - å‘¨æŠ¥ Markdown æ–‡ä»¶
    - å¯é€‰ï¼šå‘é€åˆ° Slack/Email
"""

import argparse
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any

import numpy as np
import pandas as pd

# é¡¹ç›®è·¯å¾„
SCRIPT_DIR = Path(__file__).parent
SKILL_DIR = SCRIPT_DIR.parent
DATA_DIR = SKILL_DIR / 'data'
REPORTS_DIR = SKILL_DIR / 'reports'


def main():
    parser = argparse.ArgumentParser(description='Self-Evolution Skill å‘¨æŠ¥ç”Ÿæˆå·¥å…·')
    parser.add_argument('--weeks', type=int, default=1,
                        help='ç”Ÿæˆæœ€è¿‘ N å‘¨çš„æŠ¥å‘Šï¼ˆé»˜è®¤: 1ï¼‰')
    parser.add_argument('--output', type=str,
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: reports/weekly_YYYYMMDD.mdï¼‰')
    parser.add_argument('--format', choices=['markdown', 'html', 'json'],
                        default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    args = parser.parse_args()

    # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    try:
        # ç”ŸæˆæŠ¥å‘Š
        report = generate_weekly_report(weeks=args.weeks, verbose=args.verbose)

        # ç¡®å®šè¾“å‡ºæ–‡ä»¶
        if args.output:
            output_file = Path(args.output)
        else:
            date_str = datetime.now().strftime('%Y%m%d')
            output_file = REPORTS_DIR / f"weekly_{date_str}.md"

        # ä¿å­˜æŠ¥å‘Š
        if args.format == 'markdown':
            save_markdown_report(report, output_file)
        elif args.format == 'html':
            save_html_report(report, output_file.with_suffix('.html'))
        elif args.format == 'json':
            save_json_report(report, output_file.with_suffix('.json'))

        print(f"âœ“ å‘¨æŠ¥å·²ç”Ÿæˆ: {output_file}")

    except Exception as e:
        print(f"âœ— ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def generate_weekly_report(weeks: int, verbose: bool) -> Dict[str, Any]:
    """
    ç”Ÿæˆå‘¨æŠ¥æ•°æ®

    Args:
        weeks: æŠ¥å‘Šå‘¨æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        æŠ¥å‘Šæ•°æ®å­—å…¸
    """
    if verbose:
        print(f"[1/5] åŠ è½½æ‰§è¡Œæ•°æ®...")
    executions = load_weekly_executions(weeks)

    if verbose:
        print(f"[2/5] è®¡ç®—è´¨é‡æŒ‡æ ‡...")
    quality_metrics = calculate_quality_metrics(executions)

    if verbose:
        print(f"[3/5] åˆ†ææ¨¡å¼...")
    patterns = analyze_patterns(executions)

    if verbose:
        print(f"[4/5] æ£€æŸ¥æƒé‡å˜åŒ–...")
    weights = analyze_weight_changes(weeks)

    if verbose:
        print(f"[5/5] ç”Ÿæˆæ‘˜è¦...")
    summary = generate_summary(executions, quality_metrics, patterns, weights)

    return {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'period': {
                'weeks': weeks,
                'start_date': (datetime.now() - timedelta(weeks=weeks)).strftime('%Y-%m-%d'),
                'end_date': datetime.now().strftime('%Y-%m-%d')
            }
        },
        'summary': summary,
        'quality_metrics': quality_metrics,
        'patterns': patterns,
        'weights': weights,
        'executions_count': len(executions)
    }


def load_weekly_executions(weeks: int) -> List[Dict[str, Any]]:
    """
    åŠ è½½æœ€è¿‘ N å‘¨çš„æ‰§è¡Œæ•°æ®

    Args:
        weeks: å‘¨æ•°

    Returns:
        æ‰§è¡Œè®°å½•åˆ—è¡¨
    """
    cutoff_date = datetime.now() - timedelta(weeks=weeks)
    executions = []

    exec_dir = DATA_DIR / 'executions'
    if not exec_dir.exists():
        return []

    for file_path in exec_dir.glob('*.json'):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                exec_date = datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00'))

                if exec_date >= cutoff_date:
                    executions.append(data)
        except Exception:
            continue

    return executions


def calculate_quality_metrics(executions: List[Dict]) -> Dict[str, Any]:
    """
    è®¡ç®—è´¨é‡æŒ‡æ ‡

    Args:
        executions: æ‰§è¡Œè®°å½•åˆ—è¡¨

    Returns:
        è´¨é‡æŒ‡æ ‡å­—å…¸
    """
    if not executions:
        return {
            'overall_score': 0,
            'completeness': 0,
            'consistency': 0,
            'professionalism': 0,
            'trend': 'stable'
        }

    # æŒ‰æ—¶é—´æ’åº
    sorted_execs = sorted(executions, key=lambda x: x['timestamp'])

    # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
    scores = {
        'completeness': [],
        'consistency': [],
        'professionalism': []
    }

    for exec_data in sorted_execs:
        if 'quality_score' in exec_data:
            for dimension in scores.keys():
                if dimension in exec_data['quality_score']:
                    scores[dimension].append(exec_data['quality_score'][dimension])

    # è®¡ç®—å¹³å‡å€¼
    avg_scores = {}
    for dimension, values in scores.items():
        avg_scores[dimension] = np.mean(values) if values else 0

    # è®¡ç®—æ•´ä½“åˆ†æ•°
    overall_score = np.mean(list(avg_scores.values()))

    # è®¡ç®—è¶‹åŠ¿ï¼ˆæ¯”è¾ƒå‰åŠå‘¨æœŸå’ŒååŠå‘¨æœŸï¼‰
    trend = calculate_trend(sorted_execs)

    return {
        'overall_score': round(overall_score, 2),
        'completeness': round(avg_scores['completeness'], 2),
        'consistency': round(avg_scores['consistency'], 2),
        'professionalism': round(avg_scores['professionalism'], 2),
        'trend': trend,
        'sample_size': len(sorted_execs)
    }


def calculate_trend(executions: List[Dict]) -> str:
    """
    è®¡ç®—è´¨é‡è¶‹åŠ¿

    Args:
        executions: æ‰§è¡Œè®°å½•åˆ—è¡¨ï¼ˆå·²æ’åºï¼‰

    Returns:
        è¶‹åŠ¿æè¿°ï¼š'improving', 'declining', 'stable'
    """
    if len(executions) < 10:
        return 'stable'  # æ•°æ®å¤ªå°‘ï¼Œæ— æ³•åˆ¤æ–­è¶‹åŠ¿

    mid_point = len(executions) // 2
    first_half = executions[:mid_point]
    second_half = executions[mid_point:]

    # è®¡ç®—ä¸¤åŠçš„å¹³å‡è´¨é‡åˆ†æ•°
    def avg_quality(execs):
        scores = []
        for e in execs:
            if 'quality_score' in e and 'overall' in e['quality_score']:
                scores.append(e['quality_score']['overall'])
        return np.mean(scores) if scores else 0

    first_avg = avg_quality(first_half)
    second_avg = avg_quality(second_half)

    diff = second_avg - first_avg

    if diff > 0.05:  # æå‡è¶…è¿‡ 5%
        return 'improving'
    elif diff < -0.05:  # ä¸‹é™è¶…è¿‡ 5%
        return 'declining'
    else:
        return 'stable'


def analyze_patterns(executions: List[Dict]) -> Dict[str, Any]:
    """
    åˆ†æå‘ç°çš„æ¨¡å¼

    Args:
        executions: æ‰§è¡Œè®°å½•åˆ—è¡¨

    Returns:
        æ¨¡å¼åˆ†æç»“æœ
    """
    # åŠ è½½æœ€æ–°çš„æ¨¡å¼æ–‡ä»¶
    patterns_dir = DATA_DIR / 'patterns'
    if not patterns_dir.exists():
        return {'frequent_combinations': [], 'success_patterns': []}

    latest_pattern_file = get_latest_file(patterns_dir, '*.json')
    if not latest_pattern_file:
        return {'frequent_combinations': [], 'success_patterns': []}

    try:
        with open(latest_pattern_file, 'r', encoding='utf-8') as f:
            patterns = json.load(f)
            return {
                'frequent_combinations': patterns.get('frequent_combinations', [])[:5],
                'success_patterns': patterns.get('success_patterns', [])[:5],
                'new_discoveries': count_new_patterns(patterns, weeks=1)
            }
    except Exception:
        return {'frequent_combinations': [], 'success_patterns': []}


def count_new_patterns(patterns: Dict, weeks: int) -> int:
    """
    ç»Ÿè®¡æ–°å‘ç°çš„æ¨¡å¼æ•°é‡

    Args:
        patterns: æ¨¡å¼æ•°æ®
        weeks: å‘¨æ•°

    Returns:
        æ–°æ¨¡å¼æ•°é‡
    """
    cutoff_date = datetime.now() - timedelta(weeks=weeks)
    new_count = 0

    for pattern in patterns.get('frequent_combinations', []):
        if 'first_seen' in pattern:
            try:
                first_seen = datetime.fromisoformat(pattern['first_seen'].replace('Z', '+00:00'))
                if first_seen >= cutoff_date:
                    new_count += 1
            except Exception:
                pass

    return new_count


def analyze_weight_changes(weeks: int) -> Dict[str, Any]:
    """
    åˆ†ææƒé‡å˜åŒ–

    Args:
        weeks: å‘¨æ•°

    Returns:
        æƒé‡å˜åŒ–åˆ†æ
    """
    weights_dir = DATA_DIR / 'weights'
    if not weights_dir.exists():
        return {'top_improved': [], 'top_declined': [], 'total_changes': 0}

    # è·å–æœ¬å‘¨å’Œä¸Šå‘¨çš„æƒé‡æ–‡ä»¶
    weight_files = sorted(weights_dir.glob('*.json'), reverse=True)
    if len(weight_files) < 2:
        return {'top_improved': [], 'top_declined': [], 'total_changes': 0}

    try:
        with open(weight_files[0], 'r', encoding='utf-8') as f:
            current_weights = json.load(f)
        with open(weight_files[1], 'r', encoding='utf-8') as f:
            previous_weights = json.load(f)

        # è®¡ç®—å˜åŒ–
        changes = {}
        for key, current_value in current_weights.items():
            if key in previous_weights:
                change = current_value - previous_weights[key]
                if abs(change) > 0.01:  # åªå…³æ³¨å˜åŒ–è¶…è¿‡ 1% çš„
                    changes[key] = change

        # æ’åº
        sorted_changes = sorted(changes.items(), key=lambda x: abs(x[1]), reverse=True)
        top_improved = [(k, v) for k, v in sorted_changes if v > 0][:5]
        top_declined = [(k, v) for k, v in sorted_changes if v < 0][:5]

        return {
            'top_improved': top_improved,
            'top_declined': top_declined,
            'total_changes': len(changes)
        }
    except Exception:
        return {'top_improved': [], 'top_declined': [], 'total_changes': 0}


def generate_summary(executions: List[Dict], quality_metrics: Dict,
                     patterns: Dict, weights: Dict) -> str:
    """
    ç”Ÿæˆæ‘˜è¦æ–‡æœ¬

    Args:
        executions: æ‰§è¡Œè®°å½•
        quality_metrics: è´¨é‡æŒ‡æ ‡
        patterns: æ¨¡å¼åˆ†æ
        weights: æƒé‡å˜åŒ–

    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    summary_parts = []

    # æ‰§è¡Œç»Ÿè®¡
    summary_parts.append(f"æœ¬å‘¨å…±æ‰§è¡Œ {len(executions)} æ¬¡ skill è°ƒç”¨")

    # è´¨é‡è¶‹åŠ¿
    trend_desc = {
        'improving': 'è´¨é‡æŒç»­æå‡ ğŸ“ˆ',
        'declining': 'è´¨é‡æœ‰æ‰€ä¸‹é™ ğŸ“‰',
        'stable': 'è´¨é‡ä¿æŒç¨³å®š â¡ï¸'
    }
    summary_parts.append(trend_desc[quality_metrics['trend']])

    # æ–°æ¨¡å¼
    new_patterns = patterns.get('new_discoveries', 0)
    if new_patterns > 0:
        summary_parts.append(f"å‘ç° {new_patterns} ä¸ªæ–°çš„æˆåŠŸæ¨¡å¼")

    # æƒé‡å˜åŒ–
    if weights['total_changes'] > 0:
        summary_parts.append(f"{weights['total_changes']} ä¸ªæƒé‡å‘ç”Ÿæ˜¾è‘—å˜åŒ–")

    return 'ï¼›'.join(summary_parts) + 'ã€‚'


def save_markdown_report(report: Dict, output_file: Path):
    """
    ä¿å­˜ä¸º Markdown æ ¼å¼

    Args:
        report: æŠ¥å‘Šæ•°æ®
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    md_content = generate_markdown(report)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)


def generate_markdown(report: Dict) -> str:
    """
    ç”Ÿæˆ Markdown å†…å®¹

    Args:
        report: æŠ¥å‘Šæ•°æ®

    Returns:
        Markdown æ–‡æœ¬
    """
    period = report['metadata']['period']
    summary = report['summary']
    quality = report['quality_metrics']
    patterns = report['patterns']
    weights = report['weights']

    md = f"""# Self-Evolution å‘¨æŠ¥

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
**æŠ¥å‘Šå‘¨æœŸ**: {period['start_date']} ~ {period['end_date']}

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

{summary}

- **æ€»æ‰§è¡Œæ¬¡æ•°**: {report['executions_count']}
- **æ•´ä½“è´¨é‡åˆ†æ•°**: {quality['overall_score']:.2f} / 1.00
- **è´¨é‡è¶‹åŠ¿**: {quality['trend']}

---

## ğŸ“ˆ è´¨é‡æŒ‡æ ‡

| ç»´åº¦ | åˆ†æ•° | è¯´æ˜ |
|------|------|------|
| å®Œæ•´æ€§ (Completeness) | {quality['completeness']:.2f} | ç”Ÿæˆä»£ç çš„å®Œæ•´ç¨‹åº¦ |
| ä¸€è‡´æ€§ (Consistency) | {quality['consistency']:.2f} | ä¸ç°æœ‰ä»£ç é£æ ¼çš„ä¸€è‡´æ€§ |
| ä¸“ä¸šæ€§ (Professionalism) | {quality['professionalism']:.2f} | ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µ |

**æ ·æœ¬æ•°**: {quality['sample_size']} æ¬¡æ‰§è¡Œ

---

## ğŸ” å‘ç°çš„æ¨¡å¼

### é«˜é¢‘ç»„åˆ

"""

    for i, pattern in enumerate(patterns.get('frequent_combinations', [])[:5], 1):
        items = ' + '.join(pattern.get('items', []))
        support = pattern.get('support', 0)
        md += f"{i}. **{items}** (å‡ºç° {support} æ¬¡)\n"

    md += "\n### æˆåŠŸæ¨¡å¼\n\n"

    for i, pattern in enumerate(patterns.get('success_patterns', [])[:5], 1):
        items = ' + '.join(pattern.get('items', []))
        quality_score = pattern.get('avg_quality', 0)
        md += f"{i}. **{items}** (è´¨é‡åˆ†æ•°: {quality_score:.2f})\n"

    md += f"\n**æ–°å‘ç°æ¨¡å¼**: {patterns.get('new_discoveries', 0)} ä¸ª\n"

    md += "\n---\n\n## âš–ï¸ æƒé‡å˜åŒ–\n\n### Top 5 æå‡\n\n"

    for i, (key, value) in enumerate(weights.get('top_improved', [])[:5], 1):
        md += f"{i}. **{key}**: +{value:.3f}\n"

    md += "\n### Top 5 ä¸‹é™\n\n"

    for i, (key, value) in enumerate(weights.get('top_declined', [])[:5], 1):
        md += f"{i}. **{key}**: {value:.3f}\n"

    md += f"\n**æ€»å˜åŒ–æ•°**: {weights.get('total_changes', 0)} ä¸ªæƒé‡\n"

    md += "\n---\n\n## ğŸ“Œ å»ºè®®\n\n"

    # æ ¹æ®æ•°æ®ç”Ÿæˆå»ºè®®
    if quality['trend'] == 'declining':
        md += "- âš ï¸ è´¨é‡ä¸‹é™ï¼Œå»ºè®®å®¡æŸ¥æœ€è¿‘çš„æ¨¡å¼å‘ç°å’Œæƒé‡è°ƒæ•´\n"
    elif quality['trend'] == 'improving':
        md += "- âœ“ è´¨é‡æå‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒå½“å‰ä¼˜åŒ–ç­–ç•¥\n"

    if quality['completeness'] < 0.8:
        md += "- âš ï¸ å®Œæ•´æ€§åä½ï¼Œå»ºè®®å¢å¼ºä»£ç ç”Ÿæˆçš„å®Œæ•´åº¦\n"

    if patterns.get('new_discoveries', 0) == 0:
        md += "- âš ï¸ æœªå‘ç°æ–°æ¨¡å¼ï¼Œå»ºè®®å¢åŠ æ‰§è¡Œæ ·æœ¬æˆ–è°ƒæ•´å‘ç°é˜ˆå€¼\n"

    md += "\n---\n\n*æœ¬æŠ¥å‘Šç”± Self-Evolution Skill è‡ªåŠ¨ç”Ÿæˆ*\n"

    return md


def save_html_report(report: Dict, output_file: Path):
    """ä¿å­˜ä¸º HTML æ ¼å¼"""
    # ç®€åŒ–å®ç°ï¼šå°† Markdown è½¬ä¸ºåŸºç¡€ HTML
    md_content = generate_markdown(report)
    html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Self-Evolution å‘¨æŠ¥</title>
    <style>
        body {{ font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #2c3e50; }}
        h2 {{ color: #34495e; border-bottom: 2px solid #ecf0f1; padding-bottom: 10px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #3498db; color: white; }}
    </style>
</head>
<body>
    <pre>{md_content}</pre>
</body>
</html>"""

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html)


def save_json_report(report: Dict, output_file: Path):
    """ä¿å­˜ä¸º JSON æ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)


def get_latest_file(directory: Path, pattern: str) -> Path:
    """
    è·å–ç›®å½•ä¸­æœ€æ–°çš„æ–‡ä»¶

    Args:
        directory: ç›®å½•è·¯å¾„
        pattern: æ–‡ä»¶æ¨¡å¼

    Returns:
        æœ€æ–°æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è¿”å› Noneï¼‰
    """
    files = sorted(directory.glob(pattern), key=lambda f: f.stat().st_mtime, reverse=True)
    return files[0] if files else None


if __name__ == '__main__':
    main()
